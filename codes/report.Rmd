---
title: "DA3 Assignment 1"
subtitle: 'Airbnb prediction models: New York'
output:
  html_document:
    df_print: paged
    code_download: yes
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# rm(list=ls())

library(tidyverse)
library(stargazer)
library(Hmisc)
library(kableExtra)
library(huxtable)
library(cowplot)
library(caret)
library(skimr)
library(grid)
library(glmnet)
library(xtable)
library(directlabels)
library(knitr)

```

```{r, echo=FALSE}
# install.packages("xfun")

```
## Introduction 

This is a report for Data Analysis 3: Assignment 1. The task is to perform prediction using the knowledge we obtained in the course.


As a first step, we define a business question: to help a company operating small and mid-size apartments hosting 2-6 guests. The company is set to price their new apartments not on the market. 
For this, I built a price prediction model similarly to how we did in our case study for London (Hackney borough). Also, discussion of my modeling decisions and comparison of my results is provided at the end. 
In the next step, I define the data for analysis (sample design). For this, I obtained data from [insideairbnb.com](http://insideairbnb.com/get-the-data.html) website. Data is for New York, it was compiled on 10 December, 2020. 

## Label Engineering 
We have a quantitative y - price in USD dollars. That's why I leave y as it is (without any transformation). 

## Feature engineering

### Data cleaning

The original dataset that was obtained from the website contained 36923 rows and 62 columns. I decided to remove several columns that are not used in the analysis (see Appendix). 

Then dollar signs ("$") were removed from price column. Also, false (f) and true (t) values were replaced with 1 and 0. 

Amenities column consisted of several values. These values in the row were pivoted as a column of dummy variable. List of amenities is given in Appendix. 

Code for data cleaning is provided in **data-cleaning-NY.R** file.

### Data preparation

When dataset was inspected, it was clear that "Entire apartment", "Private room in apartment", and "Entire condominium" were the most frequent property type (Appendix). Therefore, I decided to filter out the dataset and leave only observations that have these property types. 

Dummy variables were created from amenities column (that starts with 'd'). I kept dummy variables, numerical and factorial (categorical) columns and removed all other columns that are not used for the analysis. After that, I left with 49 columns (variables). 

The observations where the value for price column is missing were dropped as our analysis will be prediction of price.

I decided to narrow down my focus and filtered dataset so that it includes  observations only from Manhattan neighborhood. After that, 28402 observations decreased to 14170. 

```{r,echo=F, message=FALSE, warning=FALSE, include=FALSE}

setwd("D:/CEU/winter semester/data-analysis-3/da3_assignment1/")

source("codes/theme_bg.R")
source("codes/da_helper_functions.R")

data_in <- "data/clean/"
data_out <- "data/out/"

options(digits = 3)

data <- read_csv(paste(data_in,"airbnb_ny_cleaned.csv", sep = ""))

tbl <- table(data$property_type)
x <- sort(tbl,decreasing=T)

```

### Exploratory data analysis

We can see that minimum price is 10 dollars and maximum price is 999 dollars. Average price is 152.1 dollars. 

```{r,echo=F, message=FALSE, warning=FALSE}

file_folder <- "D:/CEU/winter semester/data-analysis-3/da3_assignment1/data/out"
data <- read_csv(paste(file_folder, "/airbnb_manhattan_workfile.csv", sep = ""))
```

```{r,echo=F, message=FALSE, warning=FALSE}
# describe(data$price)
#kable(summary(data$price))

sum_price_hux <- as_hux(summary(data$price))
sum_price_hux %>% 
  restack_across(rows = 2) %>% 
  set_caption("Summary of prices")

```

The distribution price shown below. The price is normally distributed, most of the observations having a price between 10 and 250. The plot is skewed to the right indicating some of the extreme values on the right tail (expensive apartments).

```{r,echo=F, message=FALSE, warning=FALSE}

R_F14_h_price <- ggplot(data, aes(price)) +
  geom_histogram(binwidth = 25, fill = color[1], color = color.outline, alpha = 0.8, size = 0.25) +
  ylab("count") +
  xlab("Price") +
  theme_bg()
R_F14_h_price

```

### Missing values 

I checked in which columns there are missing values. For the columns with few missing values, I imputed values as they don't have big importance. The columns inlude beds, bathrooms, minimum_nights, number_of_reviews. 

For the columns that are related to reviews, I replaced missing values (reviews) with zero when there is no review and I added flags (1 if replaced).

Currently, there are 8675 observations that are Entire apartments, 561 Entire condominiums and 4934 observations that are "Private room in apartment". 

As we are interested in the price of apartments that can accomodate 2-6 people, I  used filter so that dataset contains apartments which accommodate 2-6 people. 

Code for data preparation is provided in **data-prepare-NY.R** file.


### Functional Form

To decide what functional form to use for the models, I checked interactions between variables. 

Before choosing functional form, I looked at some descriptive statistics. 
This table shows how is the average price changing in Manhattan by property_type and room_type. 


```{r,echo=F, message=FALSE, warning=FALSE}
file_folder <- "D:/CEU/winter semester/data-analysis-3/da3_assignment1/data/out"

data <- read_csv(paste(file_folder,"/airbnb_manhattan_work.csv", sep = ""))

room_property_price <- as_hux(data %>%
  group_by(f_property_type, f_room_type) %>%
  dplyr::summarize(mean_price = mean(price, na.rm=TRUE)))
room_property_price %>% 
  set_caption("Average price by property_type, room_type")


```

The boxplot below shows that Entire home apartments are more expensive than private rooms.


```{r,echo=F, message=FALSE, warning=FALSE}

# NB all graphs, we exclude  extreme values of price
datau <- subset(data, price < 400)

## Boxplot of price by room type
g4 <- ggplot(data = datau, aes(x = f_room_type, y = price)) +
  stat_boxplot(aes(group = f_room_type), geom = "errorbar", width = 0.3,
               color = c(color[2],color[1]), size = 0.5, na.rm=T)+
  geom_boxplot(aes(group = f_room_type),
               color = c(color[2],color[1]), fill = c(color[2],color[1]),
               size = 0.5, width = 0.6, alpha = 0.3, na.rm=T, outlier.shape = NA) +
  scale_y_continuous(expand = c(0.01,0.01),limits = c(0,300), breaks = seq(0,300,100)) +
  labs(x = "Room type",y = "Price (US dollars)")+
  theme_bg()
g4
```

The boxplot below shows the variance of price based on property_type and number of people it can accommodate. 

```{r,echo=F, message=FALSE, warning=FALSE}

# Boxplot
g5 <- ggplot(datau, aes(x = factor(n_accommodates), y = price,
                        fill = factor(f_property_type), color=factor(f_property_type))) +
  geom_boxplot(alpha=0.8, na.rm=T, outlier.shape = NA, width = 0.8) +
  stat_boxplot(geom = "errorbar", width = 0.8, size = 0.3, na.rm=T)+
  scale_color_manual(name="",
                     values=c(color[2],color[1], color[3])) +
  scale_fill_manual(name="",
                    values=c(color[2],color[1], color[3])) +
  labs(x = "Accomodates (Persons)",y = "Price (US dollars)")+
  scale_y_continuous(expand = c(0.01,0.01), limits=c(0, 400), breaks = seq(0,400, 50))+
  theme_bg() +
  theme(legend.position = c(0.3,0.8)        )
g5
# can accomodate more people - more expensive
```


The most basic form for our models is linear regression (OLS). I start setting up such models in order of increasing complexity. I want to have a model as complex and as useful for prediction. Too complex model would overfit the data at hand. Therefore, I want to have a model that is complex enough to capture all the relations that is observed in the data and also useful enough to make out of sample prediction of a new apartment.  
Later. I will use LASSO - variable selection in data-driven, automated way. But now, I manually define variables. 

The following variables are used in a basic level: "n_accommodates", "n_beds", "f_property_type", "f_room_type", "n_days_since", "flag_days_since"

f_bathrooms_text variable is used as a factorized (categorical) variable.

Variable that are related to reviews will be also used for the modelling: "f_number_of_reviews","n_review_scores_rating", "flag_review_scores_rating"

Higher order variables (polynomials), "number of accomodates squared" ("n_accommodates2"),  "n_days_since2", "n_days_since3" are also used for models. 

Amenities will be used as a dummy variable.

To find appropriate interactions, we use the following boxplot. We see interaction between "Room type" and "WiFi", "Room type" and "Property type", "Room type" and "Sound system", "Property type" and "Parking", "Property type" and "Refrigerator". The boxlot shows that  interaction of only 2 interaction matters for the price change: "Property type" - "Parking" and "Room type" - "WiFi". We will use these and other dummies as interactions.


```{r,echo=F, message=FALSE, warning=FALSE}

# Basic Variables
basic_lev  <- c("n_accommodates", "n_beds", "f_property_type", "f_room_type", "n_days_since", "flag_days_since")

# Factorized variables
basic_add <- c("f_bathrooms_text")

reviews <- c("f_number_of_reviews","n_review_scores_rating", "flag_review_scores_rating")
# Higher orders
poly_lev <- c("n_accommodates2", "n_days_since2", "n_days_since3")

# Dummy variables: Extras -> collect all options and create dummies
# value = TRUE means 1. for columns that starts with d_, we have only values 0 and 1
amenities <-  grep("^d_.*", names(data), value = TRUE)

#################################################
# Look for interactions
################################################

#Look up room type interactions

p1 <- price_diff_by_variables2(data, "f_room_type", "d_wifi", "Room type", "WiFi")
p2 <- price_diff_by_variables2(data, "f_room_type", "f_property_type", "Room type", "Property type")
#Look up sound system
p4 <- price_diff_by_variables2(data, "f_room_type", "d_soundsystem", "Room type", "Sound system")
#Look up property type
p5 <- price_diff_by_variables2(data, "f_property_type", "d_parking", "Property type", "Parking")
p6 <- price_diff_by_variables2(data, "f_property_type", "d_refrigerator", "Property type", "Refrigerator")

g_interactions <- plot_grid( p1, p4, p5, p6,  nrow=2, ncol=2)
g_interactions
```


```{r,echo=F, message=FALSE, warning=FALSE}

X1  <- c("f_property_type*d_parking",  "f_room_type*d_wifi")

# Additional interactions of factors and dummies
X2  <- c("d_conditioning*f_property_type", "d_soundsystem*f_property_type", "d_wifi*f_property_type")
X3  <- c(paste0("(f_property_type + f_room_type) * (",
                paste(amenities, collapse=" + "),")"))

```

## Prediction

We create 8 models of increasing complexity: 1st model being the simples model (price ~ n_accomodates) and 8th model being the most complex (rich) model that includes a lot of interactions. 


```{r,echo=F, message=FALSE, warning=FALSE}

# Create models in levels models: 1-8
modellev1 <- " ~ n_accommodates"
modellev2 <- paste0(" ~ ",paste(basic_lev,collapse = " + "))
modellev3 <- paste0(" ~ ",paste(c(basic_lev,reviews),collapse = " + "))
modellev4 <- paste0(" ~ ",paste(c(basic_lev,reviews,poly_lev),collapse = " + "))
modellev5 <- paste0(" ~ ",paste(c(basic_lev,reviews,poly_lev,X1),collapse = " + "))
modellev6 <- paste0(" ~ ",paste(c(basic_lev,reviews,poly_lev,X1,X2),collapse = " + "))
modellev7 <- paste0(" ~ ",paste(c(basic_lev,reviews,poly_lev,X1,X2,amenities),collapse = " + "))
modellev8 <- paste0(" ~ ",paste(c(basic_lev,reviews,poly_lev,X1,X2,amenities,X3),collapse = " + "))
```

Then I separated hold-out set where I splitted the sample into 2 parts, sparing 20% of the sample to "holdout set". Holdout set is being "held out", not used at all. not for model training, not for model selection. It is used only at the end of the process, to mimic the situation of the live data (that we have never seen before while modeling). 
I use 80% data for model estimation and model selection, that is called work data. 


```{r,echo=F, message=FALSE, warning=FALSE}

# create a holdout set (20% of observations)
smp_size <- floor(0.2 * nrow(data))

# Set the random number generator: It will make results reproducible
set.seed(20180123)

# create ids:
# 1) seq_len: generate regular sequences
# 2) sample: select random rows from a table
holdout_ids <- sample(seq_len(nrow(data)), size = smp_size)
data$holdout <- 0
data$holdout[holdout_ids] <- 1

#Hold-out set Set = 2373 obs
data_holdout <- data %>% filter(holdout == 1)

#Working data set = 9495 obs
data_work <- data %>% filter(holdout == 0)
```


### Cross Validation     

I create 5 folds from the dataset. 

I partition the data into five random sets (random subsamples). This defines five ”folds:” a random subsample of 1/5 of the observations and the rest of the 4/5 of the observations.
Within each fold, the 1/5 sample is the test set, the 4/5 sample is the training set.
Later, I will take one of the folds. I estimate the 8 regression models in the training
set (that consists of 4/5 of the observations —> 80% of observations). 
I will use the estimated parameters (value of beta coefficient) to make a the prediction in the test set. Then I calculate the MSE for the prediction in the test set.
The same procedure will be repeated for the other folds. 
Then I take the average of the test-set MSE for each of the 8 regression models.
The best model - the model with the smallest cross-validated MSE will be chosen.

```{r,echo=F, message=FALSE, warning=FALSE}

n_folds=5
# Create the folds
set.seed(20180124)

# generate random integers between 1 and 5
# how many integers? the same as length of data = 9495
folds_i <- sample(rep(1:n_folds, length.out = nrow(data_work) ))

# Create results
model_results_cv <- list()

# from 1 to 8 because we have 8 models
for (i in (1:8)){
  model_name <-  paste0("modellev",i)
  model_pretty_name <- paste0("(",i,")")
  
  yvar <- "price"
  xvars <- eval(parse(text = model_name))
  formula <- formula(paste0(yvar,xvars))
  
  # Initialize values
  rmse_train <- c()
  rmse_test <- c()
  
  model_work_data <- lm(formula,data = data_work)
  BIC <- BIC(model_work_data)
  nvars <- model_work_data$rank -1
  r2 <- summary(model_work_data)$r.squared
  
  # Do the k-fold estimation
  for (k in 1:n_folds) {
    # test is where we generated 1
    # gives vector of indices where in the folds_i vector, I have 1
    # so to say: give me positions (index) of element in folds_i vector, element == 1 (k)
    test_i <- which(folds_i == k)
    # Train sample: all except test_i
    data_train <- data_work[-test_i, ]
    # Test sample
    data_test <- data_work[test_i, ]
    # Estimation and prediction
    model <- lm(formula,data = data_train)
    prediction_train <- predict(model, newdata = data_train)
    prediction_test <- predict(model, newdata = data_test)
    
    # Criteria evaluation
    rmse_train[k] <- mse_lev(prediction_train, data_train[,yvar] %>% pull)**(1/2)
    rmse_test[k] <- mse_lev(prediction_test, data_test[,yvar] %>% pull)**(1/2)
    
  }
  
  model_results_cv[[model_name]] <- list(yvar=yvar,xvars=xvars,formula=formula,model_work_data=model_work_data,
                                         rmse_train = rmse_train,rmse_test = rmse_test,BIC = BIC,
                                         model_name = model_pretty_name, nvars = nvars, r2 = r2)
}
```

Now we can compare the performance of models for each folds. 


```{r,echo=F, message=FALSE, warning=FALSE}

model <- lm(formula,data = data_train)
prediction_train <- predict(model, newdata = data_train)
prediction_test <- predict(model, newdata = data_test)

t1 <- imap(model_results_cv,  ~{
  as.data.frame(.x[c("rmse_test", "rmse_train")]) %>%
    dplyr::summarise_all(.funs = mean) %>%
    mutate("model_name" = .y , "model_pretty_name" = .x[["model_name"]] ,
           "nvars" = .x[["nvars"]], "r2" = .x[["r2"]], "BIC" = .x[["BIC"]])
}) %>%
  bind_rows()
t1

# t1_hux <- as_hux(t1)
# t1_hux %>% set_caption("Model results")
```

We can see the result of cross validation of all 8 models. The first column shows the average RMSE of each model across all 5 test sets. It is clear that Model7 has lowest BIC score and lowest RMSE. I will select this model for prediction. 

Cross validation helped us to pick the model. Now we will look into another method of feature selection. 

### LASSO

LASSO helps to select features so that the out of sample prediction contains less error. This is done in automated manner. It shrinks (penalizes) coefficients to 0 and at the end we will get necessary features. We will need tuning parameters for LASSO. 

In LASSO, we add penalty term to the linear regression to minimize sum of squared errors
penalize large coeffecients: sum of the absolute value of coefficients. At the end, some of the features will be entirely dropped from linear model

LASSO parameter - "alpha" is 1. We need lambda - how much weight do we put on this penalty term, how much do we care about having a simple model.  The larger lambda - the fewer features will be kept in linear model. We let data to speak (state) which lambda is best for our problem. We use cross validation to select among lambdas. 



```{r,echo=F, message=FALSE, warning=FALSE}

column_names <- c("Model", "N predictors", "R-squared", "BIC", "Training RMSE",
                  "Test RMSE")
# take model 8 (and find observations where there is no missing data)may
vars_model_7 <- c("price", basic_lev,reviews,poly_lev,X1,X2,amenities)
vars_model_8 <- c("price", basic_lev,reviews,poly_lev,X1,X2,amenities,X3)

# Set lasso tuning parameters
train_control <- trainControl(method = "cv", number = n_folds)
tune_grid <- expand.grid("alpha" = c(1), "lambda" = seq(0.05, 1, by = 0.05))

formula <- formula(paste0("price ~ ", paste(setdiff(vars_model_8, "price"), collapse = " + ")))

set.seed(1234)

lasso_model <- caret::train(formula,
                            data = data_work,
                            method = "glmnet",
                            preProcess = c("center", "scale"),
                            trControl = train_control,
                            tuneGrid = tune_grid,
                            na.action=na.exclude)



print(lasso_model$bestTune$lambda)
# it turns out best lambda is 0.8 
                  
```

We have estimated 20 different LASSO models. Using cross validation, out of sample performance varied. It turns out best lambda is **0.8**. 

Now, we extract LASSO coefficients (from the output of final model). Many of the coefficients are equal to 0, dropped from linear model (see Appendix). This is because of the automated feature selection that is carried out by LASSO.  


```{r,echo=F, message=FALSE, warning=FALSE}

lasso_coeffs <- coef(lasso_model$finalModel, lasso_model$bestTune$lambda) %>%
  as.matrix() %>%
  as.data.frame() %>%
  rownames_to_column(var = "variable") %>%
  rename(coefficient = `1`)  # the column has a name "1", to be renamed

```

As a next step, I filtered LASSO coefficients that are not equal to 0 (30 of them). 


```{r,echo=F, message=FALSE, warning=FALSE}

lasso_coeffs_nz<-lasso_coeffs %>%
  filter(coefficient!=0)


```


RMSE of the model that is suggested by LASSO is 85.8. It is same as RMSE of model7 - model that was selected by Cross Validation. 


```{r,echo=F, message=FALSE, warning=FALSE}
# Evaluate model. CV error:
lasso_cv_rmse <- lasso_model$results %>%
  filter(lambda == lasso_model$bestTune$lambda) %>%
  dplyr::select(RMSE)
print(lasso_cv_rmse[1, 1])

```

## Diagnsotics 

Now we use Model7 to find RMSE in holdout set. RMSE in holdout set is equal to 84.9 (close to average RMSE of Model7 in test set). This indicates that our selected model have a chance to perform well in live data. 

```{r,echo=F, message=FALSE, warning=FALSE}

model3_level <- model_results_cv[["modellev3"]][["model_work_data"]]
model7_level <- model_results_cv[["modellev7"]][["model_work_data"]]


# look at holdout RMSE
model7_level_work_rmse <- mse_lev(predict(model7_level, newdata = data_work), data_work[,"price"] %>% pull)**(1/2)
model7_level_holdout_rmse <- mse_lev(predict(model7_level, newdata = data_holdout), data_holdout[,"price"] %>% pull)**(1/2)
```


### Predicted and Actual price

 
For the average apartment, we predict a 157 dollar price. We face an uncertainty despite having a good model; prices may vary between -3 and 368 dollars. Price cannot be negative. For this, we had to constrain our model from producing negative result. 


```{r,echo=F, message=FALSE, warning=FALSE}

# Target variable

Ylev <- data_holdout[["price"]]

meanY <-mean(Ylev)
sdY <- sd(Ylev)
meanY_m2SE <- meanY -1.96 * sdY
meanY_p2SE <- meanY + 1.96 * sdY
Y5p <- quantile(Ylev, 0.05, na.rm=TRUE)
Y95p <- quantile(Ylev, 0.95, na.rm=TRUE)

# Predicted values
predictionlev_holdout_pred <- as.data.frame(predict(model7_level, newdata = data_holdout, interval="predict")) %>%
  rename(pred_lwr = lwr, pred_upr = upr)
predictionlev_holdout_conf <- as.data.frame(predict(model7_level, newdata = data_holdout, interval="confidence")) %>%
  rename(conf_lwr = lwr, conf_upr = upr)

predictionlev_holdout <- cbind(data_holdout[,c("price","n_accommodates")],
                               predictionlev_holdout_pred,
                               predictionlev_holdout_conf[,c("conf_lwr","conf_upr")])

predict_hux <- as_hux(summary(predictionlev_holdout))
predict_hux %>% set_caption("Prediction")


```


We compare predicted value with actual value (using holdout set). Dots are mainly close to 45 degree line, which indicates good prediction. 
But still, there is a lot of noice, a lot of spread in the graph.

We predicted only 4 houses with price more than 300 USD. But actually there are much more houses  with price more than 300 USD. This is quite usual phenomenon as it it difficult for regression models to predict extreme values, unless there are many of them in a very large dataset.


```{r,echo=F, message=FALSE, warning=FALSE}

# Create data frame with the real and predicted values
d <- data.frame(ylev=Ylev, predlev=predictionlev_holdout[,"fit"] )
# Check the differences
d$elev <- d$ylev - d$predlev

# Plot predicted vs price
level_vs_pred <- ggplot(data = d) +
  geom_point(aes(y=ylev, x=predlev), color = color[1], size = 1,
             shape = 16, alpha = 0.7, show.legend=FALSE, na.rm=TRUE) +
  #geom_smooth(aes(y=ylev, x=predlev), method="lm", color=color[2], se=F, size=0.8, na.rm=T)+
  geom_segment(aes(x = 0, y = 0, xend = 350, yend =350), size=0.5, color=color[2], linetype=2) +
  coord_cartesian(xlim = c(0, 350), ylim = c(0, 350)) +
  scale_x_continuous(expand = c(0.01,0.01),limits=c(0, 350), breaks=seq(0, 350, by=50)) +
  scale_y_continuous(expand = c(0.01,0.01),limits=c(0, 350), breaks=seq(0, 350, by=50)) +
  labs(y = "Price (US dollars)", x = "Predicted price  (US dollars)") +
  theme_bg() 
level_vs_pred

```

Bar chart summarizes predictive interval for apartments with different number of accommodates. Again, interval is wide because there is a lot of uncertainty. 

The conclusion from these diagnostics is that our prediction has substantial uncertainty. That uncertainty is present in all subgroups; for a five-person apartment, the 80% prediction interval is between 50 and 380 dollars. One way to reduce the prediction error is to have a larger sample size. For example, we can improve our model, by not limiting ourselves to Manhattan and using full New York City dataset. 

```{r,echo=F, message=FALSE, warning=FALSE}

predictionlev_holdout_summary <-
  predictionlev_holdout %>%
  group_by(n_accommodates) %>%
  dplyr::summarise(fit = mean(fit, na.rm=TRUE), pred_lwr = mean(pred_lwr, na.rm=TRUE), pred_upr = mean(pred_upr, na.rm=TRUE),
                   conf_lwr = mean(conf_lwr, na.rm=TRUE), conf_upr = mean(conf_upr, na.rm=TRUE))

F14_CI_n_accomodate <- ggplot(predictionlev_holdout_summary, aes(x=factor(n_accommodates))) +
  geom_bar(aes(y = fit ), stat="identity",  fill = color[1], alpha=0.7 ) +
  geom_errorbar(aes(ymin=pred_lwr, ymax=pred_upr, color = "Pred. interval"),width=.2) +
  #geom_errorbar(aes(ymin=conf_lwr, ymax=conf_upr, color = "Conf. interval"),width=.2) +
  scale_y_continuous(name = "Predicted price (US dollars)") +
  scale_x_discrete(name = "Accomodates (Persons)") +
  scale_color_manual(values=c(color[2], color[2])) +
  theme_bg() +
  theme(legend.title= element_blank(),legend.position="none")
F14_CI_n_accomodate

```


## Appendix

Code repo: https://github.com/abdu95/da3_assignment1


### Data Cleaning

Removed columns: 
  
  
  "host_thumbnail_url","host_picture_url","listing_url","thumbnail_url","medium_url","picture_url","xl_picture_url","host_url","last_scraped","description", "experiences_offered", "neighborhood_overview", "notes", "transit", "access", "interaction", "house_rules", "host_about", "host_response_time", "name", "summary", "space", "host_location"


Amenities:
  
  "conditioner", "stove", "hdtv", "tv", "conditioning", 
"sound system", "refrigerator", "shampoo", "soap", 
"oven", "toiletries", "speaker", "fan", "heating", 
"breakfast", "table", "dishwasher", "dryer", "elevator", 
"fitness", "parking", "garage", "wifi", "game", "garden", 
"gym", "restaurant", "bar", "washer", "barbeque", "bbq"


Property types:

```{r,echo=F, message=FALSE, warning=FALSE}

properties_hux <- head(as_hux(x), 6)
properties_hux %>% set_caption("Property types")

```

### LASSO


```{r,echo=F, message=FALSE, warning=FALSE}

print(lasso_coeffs)
```
